completed loading Train dataset
completed loading Val dataset
begin training
10th iter 	 loss: 0.4427250027656555
20th iter 	 loss: 0.3969710171222687
30th iter 	 loss: 0.376956045627594
40th iter 	 loss: 0.3381091356277466
50th iter 	 loss: 0.3383881449699402
60th iter 	 loss: 0.3275883197784424
70th iter 	 loss: 0.33989256620407104
80th iter 	 loss: 0.33609738945961
90th iter 	 loss: 0.3299117684364319
100th iter 	 loss: 0.31968194246292114
110th iter 	 loss: 0.32061707973480225
120th iter 	 loss: 0.31070101261138916
130th iter 	 loss: 0.30376261472702026
140th iter 	 loss: 0.30884701013565063
150th iter 	 loss: 0.3147753179073334
160th iter 	 loss: 0.292309433221817
170th iter 	 loss: 0.3058146834373474
180th iter 	 loss: 0.2978841960430145
190th iter 	 loss: 0.30283012986183167
200th iter 	 loss: 0.29594454169273376
210th iter 	 loss: 0.28370797634124756
220th iter 	 loss: 0.29282456636428833
230th iter 	 loss: 0.29898694157600403
240th iter 	 loss: 0.2962556779384613
250th iter 	 loss: 0.3081515431404114
260th iter 	 loss: 0.28766971826553345
270th iter 	 loss: 0.2829478979110718
280th iter 	 loss: 0.2858063578605652
290th iter 	 loss: 0.2808748185634613
300th iter 	 loss: 0.2883923053741455
310th iter 	 loss: 0.2843484878540039
320th iter 	 loss: 0.28643909096717834
330th iter 	 loss: 0.2896440029144287
340th iter 	 loss: 0.29423031210899353
350th iter 	 loss: 0.2795392572879791
360th iter 	 loss: 0.2984531819820404
370th iter 	 loss: 0.2863209843635559
380th iter 	 loss: 0.2832558751106262
390th iter 	 loss: 0.30096614360809326
400th iter 	 loss: 0.28600186109542847
410th iter 	 loss: 0.2771053910255432
420th iter 	 loss: 0.28425049781799316
430th iter 	 loss: 0.2799729108810425
440th iter 	 loss: 0.29347333312034607
450th iter 	 loss: 0.2856101095676422
460th iter 	 loss: 0.28246334195137024
470th iter 	 loss: 0.2866728901863098
480th iter 	 loss: 0.2765747904777527
490th iter 	 loss: 0.28246009349823
500th iter 	 loss: 0.2861933708190918
510th iter 	 loss: 0.2808752655982971
520th iter 	 loss: 0.2867305874824524
530th iter 	 loss: 0.2805563807487488
540th iter 	 loss: 0.29308292269706726
550th iter 	 loss: 0.2816362679004669
560th iter 	 loss: 0.27786558866500854
570th iter 	 loss: 0.29105567932128906
580th iter 	 loss: 0.29241856932640076
590th iter 	 loss: 0.2913723587989807
600th iter 	 loss: 0.2957790493965149
610th iter 	 loss: 0.2838746905326843
620th iter 	 loss: 0.28780320286750793
630th iter 	 loss: 0.2889265716075897
640th iter 	 loss: 0.28474515676498413
650th iter 	 loss: 0.2812030613422394
660th iter 	 loss: 0.2941751480102539
670th iter 	 loss: 0.2863253057003021
680th iter 	 loss: 0.27028733491897583
690th iter 	 loss: 0.28116390109062195
700th iter 	 loss: 0.2909032702445984
710th iter 	 loss: 0.29233813285827637
720th iter 	 loss: 0.27820220589637756
730th iter 	 loss: 0.283738911151886
740th iter 	 loss: 0.26879268884658813
750th iter 	 loss: 0.2744242548942566
760th iter 	 loss: 0.2836560904979706
770th iter 	 loss: 0.29571112990379333
780th iter 	 loss: 0.2909523546695709
790th iter 	 loss: 0.27911487221717834
800th iter 	 loss: 0.277852863073349
810th iter 	 loss: 0.28050562739372253
820th iter 	 loss: 0.28444626927375793
830th iter 	 loss: 0.294495165348053
840th iter 	 loss: 0.29399794340133667
850th iter 	 loss: 0.27498292922973633
860th iter 	 loss: 0.27532440423965454
870th iter 	 loss: 0.281532883644104
880th iter 	 loss: 0.26459282636642456
890th iter 	 loss: 0.2891363799571991
900th iter 	 loss: 0.2800043523311615
910th iter 	 loss: 0.2721376121044159
920th iter 	 loss: 0.2936384081840515
930th iter 	 loss: 0.2879839241504669
940th iter 	 loss: 0.2951499819755554
950th iter 	 loss: 0.2809196710586548
960th iter 	 loss: 0.28357118368148804
970th iter 	 loss: 0.27808135747909546
980th iter 	 loss: 0.27297064661979675
990th iter 	 loss: 0.2784346640110016
1000th iter 	 loss: 0.2719816565513611
1010th iter 	 loss: 0.29775768518447876
