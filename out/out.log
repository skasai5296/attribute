completed loading Train dataset
completed loading Val dataset
begin training
10th iter 	 loss: 0.4417533874511719
20th iter 	 loss: 0.41753965616226196
30th iter 	 loss: 0.36028167605400085
40th iter 	 loss: 0.36688661575317383
50th iter 	 loss: 0.34589800238609314
60th iter 	 loss: 0.3249550759792328
70th iter 	 loss: 0.32915541529655457
80th iter 	 loss: 0.3261128067970276
90th iter 	 loss: 0.3197205066680908
100th iter 	 loss: 0.3213879466056824
110th iter 	 loss: 0.3013024628162384
120th iter 	 loss: 0.31828686594963074
130th iter 	 loss: 0.3214718699455261
140th iter 	 loss: 0.3112422823905945
150th iter 	 loss: 0.29855531454086304
160th iter 	 loss: 0.30188679695129395
170th iter 	 loss: 0.3140844404697418
180th iter 	 loss: 0.3075711131095886
190th iter 	 loss: 0.300312340259552
200th iter 	 loss: 0.3102012872695923
210th iter 	 loss: 0.29657262563705444
220th iter 	 loss: 0.30381321907043457
230th iter 	 loss: 0.3043566346168518
240th iter 	 loss: 0.2980780005455017
250th iter 	 loss: 0.2974068224430084
260th iter 	 loss: 0.30548661947250366
270th iter 	 loss: 0.2971881031990051
280th iter 	 loss: 0.29146894812583923
290th iter 	 loss: 0.29833292961120605
300th iter 	 loss: 0.31133633852005005
310th iter 	 loss: 0.2823578715324402
320th iter 	 loss: 0.2966511845588684
330th iter 	 loss: 0.27997034788131714
340th iter 	 loss: 0.2914261221885681
350th iter 	 loss: 0.2824016213417053
360th iter 	 loss: 0.2876282036304474
370th iter 	 loss: 0.29379168152809143
380th iter 	 loss: 0.27691712975502014
390th iter 	 loss: 0.287835031747818
400th iter 	 loss: 0.29277002811431885
410th iter 	 loss: 0.3003092408180237
420th iter 	 loss: 0.2876272201538086
430th iter 	 loss: 0.2909300923347473
440th iter 	 loss: 0.27972549200057983
450th iter 	 loss: 0.3052915930747986
460th iter 	 loss: 0.28903496265411377
470th iter 	 loss: 0.28061363101005554
480th iter 	 loss: 0.2807749807834625
490th iter 	 loss: 0.2864784598350525
500th iter 	 loss: 0.28339797258377075
510th iter 	 loss: 0.27479228377342224
520th iter 	 loss: 0.2802342176437378
530th iter 	 loss: 0.280752956867218
540th iter 	 loss: 0.2884193956851959
550th iter 	 loss: 0.28763410449028015
560th iter 	 loss: 0.2935819625854492
570th iter 	 loss: 0.29389506578445435
580th iter 	 loss: 0.29553383588790894
590th iter 	 loss: 0.28464552760124207
600th iter 	 loss: 0.29161641001701355
610th iter 	 loss: 0.2852274477481842
620th iter 	 loss: 0.2958812415599823
630th iter 	 loss: 0.28756043314933777
640th iter 	 loss: 0.2779901623725891
650th iter 	 loss: 0.2868434488773346
660th iter 	 loss: 0.29354128241539
670th iter 	 loss: 0.2865689694881439
680th iter 	 loss: 0.28685396909713745
690th iter 	 loss: 0.27741125226020813
700th iter 	 loss: 0.28461912274360657
710th iter 	 loss: 0.28220364451408386
720th iter 	 loss: 0.2802809476852417
730th iter 	 loss: 0.276807963848114
740th iter 	 loss: 0.2792070508003235
750th iter 	 loss: 0.2897031903266907
760th iter 	 loss: 0.27903053164482117
770th iter 	 loss: 0.27852529287338257
780th iter 	 loss: 0.3023587167263031
790th iter 	 loss: 0.28023332357406616
800th iter 	 loss: 0.27341094613075256
810th iter 	 loss: 0.3024098873138428
820th iter 	 loss: 0.27850013971328735
830th iter 	 loss: 0.28765934705734253
840th iter 	 loss: 0.2823910415172577
850th iter 	 loss: 0.2782629728317261
860th iter 	 loss: 0.26352134346961975
870th iter 	 loss: 0.2791386544704437
880th iter 	 loss: 0.2826566994190216
890th iter 	 loss: 0.29040807485580444
900th iter 	 loss: 0.27883777022361755
910th iter 	 loss: 0.28588855266571045
920th iter 	 loss: 0.2802414000034332
930th iter 	 loss: 0.28452375531196594
940th iter 	 loss: 0.27251872420310974
950th iter 	 loss: 0.26054462790489197
960th iter 	 loss: 0.2691234052181244
970th iter 	 loss: 0.2797064185142517
980th iter 	 loss: 0.2920764088630676
990th iter 	 loss: 0.26626116037368774
1000th iter 	 loss: 0.2797527313232422
1010th iter 	 loss: 0.28497448563575745
1020th iter 	 loss: 0.2857421338558197
1030th iter 	 loss: 0.2862201929092407
1040th iter 	 loss: 0.2803434729576111
1050th iter 	 loss: 0.3017081320285797
1060th iter 	 loss: 0.28103604912757874
1070th iter 	 loss: 0.27787598967552185
1080th iter 	 loss: 0.2709888815879822
1090th iter 	 loss: 0.2741679549217224
1100th iter 	 loss: 0.2633265256881714
1110th iter 	 loss: 0.2719328999519348
1120th iter 	 loss: 0.2805244028568268
1130th iter 	 loss: 0.2678796052932739
1140th iter 	 loss: 0.28616636991500854
1150th iter 	 loss: 0.2746054232120514
1160th iter 	 loss: 0.28210192918777466
1170th iter 	 loss: 0.29051369428634644
1180th iter 	 loss: 0.2835216820240021
1190th iter 	 loss: 0.2822129726409912
1200th iter 	 loss: 0.2558708190917969
1210th iter 	 loss: 0.2818697392940521
1220th iter 	 loss: 0.2763225734233856
1230th iter 	 loss: 0.2716701626777649
1240th iter 	 loss: 0.313751757144928
1250th iter 	 loss: 0.27803635597229004
1260th iter 	 loss: 0.27579039335250854
------------------------------
epoch [0/3] done | accuracy [0/1408]
------------------------------
10th iter 	 loss: 0.29255983233451843
20th iter 	 loss: 0.27826249599456787
30th iter 	 loss: 0.26774662733078003
40th iter 	 loss: 0.26778584718704224
50th iter 	 loss: 0.27790021896362305
60th iter 	 loss: 0.28160974383354187
70th iter 	 loss: 0.2725614011287689
80th iter 	 loss: 0.2754301130771637
90th iter 	 loss: 0.26785144209861755
100th iter 	 loss: 0.2666439414024353
110th iter 	 loss: 0.2788309156894684
120th iter 	 loss: 0.285616010427475
130th iter 	 loss: 0.266011118888855
140th iter 	 loss: 0.2682795524597168
150th iter 	 loss: 0.276489794254303
160th iter 	 loss: 0.2663409113883972
170th iter 	 loss: 0.264901340007782
180th iter 	 loss: 0.2655361294746399
190th iter 	 loss: 0.2743569016456604
200th iter 	 loss: 0.2812928557395935
210th iter 	 loss: 0.2749660909175873
220th iter 	 loss: 0.2670425772666931
230th iter 	 loss: 0.2751026749610901
240th iter 	 loss: 0.26903611421585083
250th iter 	 loss: 0.2751755714416504
260th iter 	 loss: 0.27583572268486023
270th iter 	 loss: 0.2710658311843872
280th iter 	 loss: 0.2784382402896881
290th iter 	 loss: 0.27495187520980835
300th iter 	 loss: 0.27530282735824585
310th iter 	 loss: 0.27208229899406433
320th iter 	 loss: 0.2613688111305237
330th iter 	 loss: 0.28687605261802673
340th iter 	 loss: 0.2865031659603119
350th iter 	 loss: 0.26036348938941956
360th iter 	 loss: 0.2751442790031433
370th iter 	 loss: 0.28876179456710815
380th iter 	 loss: 0.2670726180076599
390th iter 	 loss: 0.2816227376461029
400th iter 	 loss: 0.2736319303512573
410th iter 	 loss: 0.278175413608551
420th iter 	 loss: 0.2826615571975708
430th iter 	 loss: 0.26386186480522156
440th iter 	 loss: 0.28652939200401306
450th iter 	 loss: 0.2696467936038971
460th iter 	 loss: 0.2703307569026947
470th iter 	 loss: 0.2695080041885376
480th iter 	 loss: 0.2784181833267212
490th iter 	 loss: 0.280752569437027
500th iter 	 loss: 0.27313855290412903
510th iter 	 loss: 0.2861282527446747
520th iter 	 loss: 0.2864378094673157
530th iter 	 loss: 0.26232457160949707
540th iter 	 loss: 0.2741643488407135
550th iter 	 loss: 0.2635701596736908
560th iter 	 loss: 0.26893720030784607
570th iter 	 loss: 0.29141807556152344
580th iter 	 loss: 0.25735777616500854
590th iter 	 loss: 0.25613927841186523
600th iter 	 loss: 0.26279664039611816
610th iter 	 loss: 0.2596893906593323
620th iter 	 loss: 0.2736215889453888
630th iter 	 loss: 0.2676517963409424
640th iter 	 loss: 0.26628392934799194
650th iter 	 loss: 0.27632373571395874
660th iter 	 loss: 0.27021920680999756
670th iter 	 loss: 0.2710708975791931
680th iter 	 loss: 0.27804243564605713
690th iter 	 loss: 0.27869346737861633
700th iter 	 loss: 0.27245983481407166
710th iter 	 loss: 0.2729126214981079
720th iter 	 loss: 0.2933933734893799
730th iter 	 loss: 0.2824334502220154
740th iter 	 loss: 0.2876204550266266
750th iter 	 loss: 0.2724296450614929
760th iter 	 loss: 0.2842974364757538
770th iter 	 loss: 0.2805732190608978
780th iter 	 loss: 0.26945972442626953
790th iter 	 loss: 0.2815820574760437
800th iter 	 loss: 0.28095221519470215
810th iter 	 loss: 0.27358561754226685
820th iter 	 loss: 0.276170939207077
830th iter 	 loss: 0.2720611095428467
840th iter 	 loss: 0.287148654460907
850th iter 	 loss: 0.26449668407440186
860th iter 	 loss: 0.28121811151504517
870th iter 	 loss: 0.2796686887741089
880th iter 	 loss: 0.2621695399284363
890th iter 	 loss: 0.2811945974826813
900th iter 	 loss: 0.2788638472557068
910th iter 	 loss: 0.2893807888031006
920th iter 	 loss: 0.2848623991012573
930th iter 	 loss: 0.2637304961681366
940th iter 	 loss: 0.26505592465400696
950th iter 	 loss: 0.2642223834991455
960th iter 	 loss: 0.27645736932754517
970th iter 	 loss: 0.27393457293510437
980th iter 	 loss: 0.26501211524009705
990th iter 	 loss: 0.2741355895996094
1000th iter 	 loss: 0.2694212794303894
1010th iter 	 loss: 0.2706020474433899
1020th iter 	 loss: 0.2771450877189636
1030th iter 	 loss: 0.2958095967769623
1040th iter 	 loss: 0.26086902618408203
1050th iter 	 loss: 0.27279454469680786
1060th iter 	 loss: 0.2809632420539856
1070th iter 	 loss: 0.2682698369026184
1080th iter 	 loss: 0.26975035667419434
1090th iter 	 loss: 0.27300748229026794
1100th iter 	 loss: 0.26937180757522583
1110th iter 	 loss: 0.2627887725830078
1120th iter 	 loss: 0.2655143141746521
1130th iter 	 loss: 0.2811071276664734
1140th iter 	 loss: 0.2711864113807678
1150th iter 	 loss: 0.2685251832008362
1160th iter 	 loss: 0.2727661728858948
1170th iter 	 loss: 0.27536946535110474
1180th iter 	 loss: 0.251952201128006
1190th iter 	 loss: 0.25737565755844116
1200th iter 	 loss: 0.2577954828739166
1210th iter 	 loss: 0.26284486055374146
1220th iter 	 loss: 0.2762133777141571
1230th iter 	 loss: 0.27542179822921753
1240th iter 	 loss: 0.2831399440765381
1250th iter 	 loss: 0.28671717643737793
1260th iter 	 loss: 0.2731216549873352
------------------------------
epoch [1/3] done | accuracy [0/1408]
------------------------------
10th iter 	 loss: 0.2588786780834198
20th iter 	 loss: 0.26874178647994995
30th iter 	 loss: 0.29366952180862427
40th iter 	 loss: 0.26053136587142944
50th iter 	 loss: 0.2577390968799591
60th iter 	 loss: 0.26264601945877075
70th iter 	 loss: 0.27099254727363586
80th iter 	 loss: 0.2599804103374481
90th iter 	 loss: 0.28319916129112244
100th iter 	 loss: 0.2684534192085266
110th iter 	 loss: 0.26910901069641113
120th iter 	 loss: 0.2804349362850189
130th iter 	 loss: 0.255028635263443
140th iter 	 loss: 0.2587581276893616
150th iter 	 loss: 0.27980899810791016
160th iter 	 loss: 0.26958397030830383
170th iter 	 loss: 0.2599342465400696
180th iter 	 loss: 0.26510676741600037
190th iter 	 loss: 0.2805210053920746
200th iter 	 loss: 0.2756693959236145
210th iter 	 loss: 0.2657800316810608
220th iter 	 loss: 0.2671657204627991
230th iter 	 loss: 0.2891923785209656
240th iter 	 loss: 0.2936541736125946
250th iter 	 loss: 0.2740653157234192
260th iter 	 loss: 0.2886486351490021
270th iter 	 loss: 0.2737356126308441
280th iter 	 loss: 0.2597826421260834
290th iter 	 loss: 0.2804158926010132
300th iter 	 loss: 0.2683585286140442
310th iter 	 loss: 0.27000299096107483
320th iter 	 loss: 0.2791253328323364
330th iter 	 loss: 0.26545777916908264
340th iter 	 loss: 0.2800595462322235
350th iter 	 loss: 0.2704988718032837
360th iter 	 loss: 0.25886285305023193
370th iter 	 loss: 0.24903829395771027
380th iter 	 loss: 0.26276320219039917
390th iter 	 loss: 0.26296910643577576
400th iter 	 loss: 0.2571779191493988
410th iter 	 loss: 0.27631473541259766
420th iter 	 loss: 0.26338857412338257
430th iter 	 loss: 0.2526478171348572
440th iter 	 loss: 0.2806001305580139
450th iter 	 loss: 0.27581456303596497
460th iter 	 loss: 0.26628589630126953
470th iter 	 loss: 0.28020864725112915
480th iter 	 loss: 0.2701036036014557
490th iter 	 loss: 0.2596948742866516
500th iter 	 loss: 0.2603306770324707
510th iter 	 loss: 0.2732016146183014
520th iter 	 loss: 0.27806490659713745
530th iter 	 loss: 0.2691146731376648
540th iter 	 loss: 0.2571966350078583
550th iter 	 loss: 0.2741234600543976
560th iter 	 loss: 0.26042789220809937
570th iter 	 loss: 0.271292120218277
580th iter 	 loss: 0.28792497515678406
590th iter 	 loss: 0.2689135670661926
600th iter 	 loss: 0.27038702368736267
610th iter 	 loss: 0.2494153529405594
620th iter 	 loss: 0.2729059159755707
630th iter 	 loss: 0.2669345736503601
640th iter 	 loss: 0.2742663025856018
650th iter 	 loss: 0.2822068929672241
660th iter 	 loss: 0.27591341733932495
670th iter 	 loss: 0.2748557925224304
680th iter 	 loss: 0.2601858675479889
690th iter 	 loss: 0.2708999514579773
700th iter 	 loss: 0.27029135823249817
710th iter 	 loss: 0.2696596384048462
720th iter 	 loss: 0.2681248188018799
730th iter 	 loss: 0.2617272734642029
740th iter 	 loss: 0.2618615925312042
750th iter 	 loss: 0.27265435457229614
760th iter 	 loss: 0.2704419195652008
770th iter 	 loss: 0.2668209373950958
780th iter 	 loss: 0.2636944353580475
790th iter 	 loss: 0.26525434851646423
800th iter 	 loss: 0.2766816020011902
810th iter 	 loss: 0.2675412595272064
820th iter 	 loss: 0.2842470109462738
830th iter 	 loss: 0.2772987484931946
840th iter 	 loss: 0.2628174126148224
850th iter 	 loss: 0.26139703392982483
860th iter 	 loss: 0.26492953300476074
870th iter 	 loss: 0.27239343523979187
880th iter 	 loss: 0.2750716507434845
890th iter 	 loss: 0.261393666267395
900th iter 	 loss: 0.2697950601577759
910th iter 	 loss: 0.27213504910469055
920th iter 	 loss: 0.2659740447998047
930th iter 	 loss: 0.2749890387058258
940th iter 	 loss: 0.27793198823928833
950th iter 	 loss: 0.26135849952697754
960th iter 	 loss: 0.26528164744377136
970th iter 	 loss: 0.2578164339065552
980th iter 	 loss: 0.2759327292442322
990th iter 	 loss: 0.26641029119491577
1000th iter 	 loss: 0.2579076588153839
1010th iter 	 loss: 0.27868369221687317
1020th iter 	 loss: 0.2846655249595642
1030th iter 	 loss: 0.2719511091709137
1040th iter 	 loss: 0.27306583523750305
1050th iter 	 loss: 0.2667306363582611
1060th iter 	 loss: 0.2802774906158447
1070th iter 	 loss: 0.2584960162639618
1080th iter 	 loss: 0.2714709937572479
1090th iter 	 loss: 0.259668231010437
1100th iter 	 loss: 0.27316388487815857
1110th iter 	 loss: 0.26475149393081665
1120th iter 	 loss: 0.25433629751205444
1130th iter 	 loss: 0.2770639657974243
1140th iter 	 loss: 0.2786289155483246
1150th iter 	 loss: 0.27773094177246094
1160th iter 	 loss: 0.2592073678970337
1170th iter 	 loss: 0.2593957781791687
1180th iter 	 loss: 0.2688160240650177
1190th iter 	 loss: 0.2655204236507416
1200th iter 	 loss: 0.2786667048931122
1210th iter 	 loss: 0.26696568727493286
1220th iter 	 loss: 0.26877084374427795
1230th iter 	 loss: 0.2763753831386566
1240th iter 	 loss: 0.2736022472381592
1250th iter 	 loss: 0.2883594334125519
1260th iter 	 loss: 0.28764820098876953
------------------------------
epoch [2/3] done | accuracy [0/1408]
------------------------------
end training
